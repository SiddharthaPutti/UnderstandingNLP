<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Low-Rank Adapters | Sid&#39;IO</title>
<meta name="keywords" content="nlp, language-model, alignment, steerability, prompting" />
<meta name="description" content="All this is going to be in lame/understandable language.">
<meta name="author" content="Siddhartha Putti">
<link rel="canonical" href="https://siddharthaputti.github.io//posts/LoRA/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css" integrity="sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://siddharthaputti.github.io/doge.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://siddharthaputti.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://siddharthaputti.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://siddharthaputti.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://siddharthaputti.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<style>
    .code-tabs {
        margin: 20px 0;
        border: 1px solid var(--border);
        border-radius: 4px;
    }
    
    .tab-buttons {
        display: flex;
        border-bottom: 1px solid var(--border);
        background: var(--entry);
    }
    
    .tab-button {
        padding: 10px 20px;
        border: none;
        background: none;
        color: var(--secondary);
        cursor: pointer;
        border-right: 1px solid var(--border);
    }
    
    .tab-button:hover {
        background: var(--tertiary);
    }
    
    .tab-button.active {
        background: var(--theme);
        color: var(--primary);
    }
    
    .tab-content {
        display: none;
        padding: 15px;
    }
    
    .tab-content.active {
        display: block;
    }
    
    .tab-content pre {
        margin: 0;
    }
    </style>
    
    <script>
    function showCode(tabId) {
        // Hide all tab contents
        document.querySelectorAll('.tab-content').forEach(tab => {
            tab.classList.remove('active');
        });
        
        // Deactivate all buttons
        document.querySelectorAll('.tab-button').forEach(button => {
            button.classList.remove('active');
        });
        
        // Show selected tab content
        document.getElementById(tabId).classList.add('active');
        
        // Activate selected button
        document.querySelector(`[onclick="showCode('${tabId}')"]`).classList.add('active');
    }
    </script>
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-HFT45VFBX6', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Low-Rank Adapters" />
<meta property="og:description" content="LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://siddharthaputti.github.io//posts/LoRA/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-15T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-03-15T00:00:00&#43;00:00" />

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://siddharthaputti.github.io//posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Word Embedding",
      "item": "https://siddharthaputti.github.io//posts/LoRA/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LoRA",
  "name": "LoRA",
  "description": "LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS",
  "keywords": [
    "nlp", "language-model", "alignment", "steerability", "prompting"
  ],
  "articleBody": "",
   "wordCount" : "4428",
  "inLanguage": "en",
  "datePublished": "2023-03-15T00:00:00Z",
  "dateModified": "2023-03-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Siddhartha Putti"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://siddharthaputti.github.io//posts/LoRA/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sid'IO",
    "logo": {
      "@type": "ImageObject",
      "url": "https://siddharthaputti.github.io/doge.png"
    }
  }
}

</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://siddharthaputti.github.io/" accesskey="h" title="Sid&#39;IO (Alt + H)">Sid&#39;IO</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://siddharthaputti.github.io/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
           
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      ABC's of CUDA.
    </h1>
    <div class="post-meta"><span title='2023-03-15 00:00:00 +0000 UTC'>Dec 17, 2023</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp; Siddhartha Putti
    </div>
    
    <h2 style="margin-top: 40px">GPU accelerated VS CPU-only Applications</h2>

<li>Let's begin with a discussion of some of the key differences between GPU accelerated as compared to CPU-only applications.</li>
<div style="margin: 20px 0">
    <img src="cpu_timeline.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
</div>
<li>The above picture, you can see a timeline that indicates work that can be done on CPU and representation of data that will need to initalized and and worked on in the application. 
    In CPU-only applications, data, of course, is allocated on the CPU after the data has been allocated and or initialized, we're then able to perform work on the CPU on that data. You can see, 
    based on the timeline, that the work is typically performed serially, which is to say one item of work will proceed After the previous item of work and so on and so forth. 
    If we have any downstream tasks after performing the work, such as verifying the results of our processing, then that work is also performed on the CPU. </li>

    <div style="margin: 20px 0">
        <img src="gpu_timeline.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
    </div>

    <li>we've made a modification now that we want to think about a GPU-accelerated application, and the way that we have to think about this is that 
        we have two separate devices that we're going to be working with. We have the traditional CPU, and we also have the GPU device. </li>
    <li>In a GPU-accelerated application, we can allocate our data, and that data will be initially available on the CPU. we are going to be 
        using a call to cudaMallocManaged() to allocate our data. Here, once we have allocated it with cudaMallocManaged, we can still perform 
        functions such as the initialization of the values in the data on the CPU. </li>
    <li>However, now when we wish to perform work, we can indicate that we wish to perform the work on the GPU, where, as is represented 
        in this picture above, it will be performed largely in parallel. Having used cudaMallocManaged to allocate the data, 
        the data will automatically be migrated to the GPU, where the parallel work can be done. </li>
    <li>Worth mentioning that work on the GPU is asynchronous to the work that could be happening on the CPU at the same time. 
        To represent that in this picture, we've shown that while the GPU is performing its work in parallel, it may be the case that 
        we wish to do some other work on the CPU at the same time. It's going to be important at times to synchronize our CPU code with 
        the completion of the work or some of the work that's happening on the GPU, and we can do that by creating synchronization barriers. 
        You'll be looking at a particular way to do that using a call to cudaDeviceSynchronize. Let's assume for the sake of example that we 
        then wish to do some downstream verification of our GPU work on the CPU. Having used cudaMallocManaged to allocate our data, the data 
        accesses by the CPU will automatically be migrated back to the CPU. </li>

<h2 style="margin-top: 40px">CUDA kernel execution</h2>
    <li>Now, let's look at some of the key terms and concepts associated with launching CUDA kernels, and some of the initial details to help you understand 
        how they perform their parallel work. In the previous section, we looked at when we perform work in the context of an accelerated application that 
        involves both a CPU and a GPU, and we showed that when we perform work, we can perform it in parallel on the GPU. Here, we're going to zoom in to 
        this call to a function we show here as PerformWork. </li> 

        <div style="margin: 20px 0">
            <img src="gpu-arch_1.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
        </div>

    <li>So, as mentioned, and as one of the great strengths of GPU parallel programming, GPUs are going to do work in a massively parallel way. 
        When we perform work on the GPU, the GPU work is done in something we call a thread. In this image, and in many of the images to follow, 
        we are representing a thread of work by one of these white rectangles. Many threads, in fact, on the GPU will run in a massively parallel fashion. 
        There is a way to logically group threads, and we call this logical collection of threads a block. </li> 
    <li>So, a block consists of one or more threads. We call a collection of all the blocks associated with work on the GPU a grid. 
        So, we have a grid of blocks of threads. When we refer to CUDA functions that run on the GPU, we have a special name for them. 
        They are called kernels. In this image, we have a kernel called PerformWork. When we invoke kernels, we say that we launch them, 
        and different from CPU-only functions, we launch them with something called an execution configuration, which you can see here in green. 
        It is this triple chevron syntax that we provide after the name of the function and before the parentheses, where we may need to pass arguments 
        to the function that it would be expected to have. </li> 
    <li>The execution configuration defines the number of blocks in the grid. This is the first argument that we pass into the execution configuration. 
        In this image, you can see that we are defining there should be two blocks in the grid. The execution configuration also defines the number of 
        threads that should be in each block. So, here we are stating that we should have four threads in each of our two blocks. 
        Worth mentioning is that every block in the grid will contain the same number of threads. </li>
    <li>To summarize, The kernel function is defined using the __global__ keyword, and it takes in a variable number of arguments. The arguments are passed to the kernel
         function using the <<<>>> syntax, where the first argument is the number of blocks in the grid, and the second argument is the number of threads 
         in the block.</li>



         
         <div class="code-tabs">
            <div class="tab-buttons">
                <button class="tab-button active" onclick="showCode('cuda')">simple cuda kernel</button>
                <button class="tab-button" onclick="showCode('model')">Parallel CUDA kernel</button>
            </div>
        
            <!-- CUDA Example Tab -->
            <div id="cuda" class="tab-content active">
                <pre><code class="language-c">
        #include stdio.h
        
        void helloCPU()
        {
            printf("Hello from the CPU.\n");
        }
        
        __global__ void helloGPU()
        {
            printf("Hello from the GPU\n");
        }
        
        int main()
        {
            helloCPU();
            helloGPU<<<1, 1>>>();
            cudaDeviceSynchronize();
        }
                </code></pre>
        </div>
    
        <!-- Model Definition Tab -->
        <div id="model" class="tab-content">
            <pre><code class="language-python">
            #include stdio.h

            __global__ void firstParallel()
            {
                printf("This should be running in parallel.\n");
            }
            
            int main()
            {

                firstParallel<<<5, 5>>>(); # this will run 25 times in parallel. 
                cudaDeviceSynchronize();
            
            }
                </code></pre>
            </div>

        </div>

<h2 style="margin-top: 40px">CUDA provided thread hierarchy varaibles.</h2>
        <div style="margin: 20px 0">
            <img src="gpu_grid.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
        </div>

        <li>Now we're going to look at some CUDA-provided thread hierarchy variables that are made available to us by the CUDA runtime to use within 
            our CUDA kernel definitions and help control how we perform the parallel work that we will do with our kernel on the GPU. 
            We looked in a previous section at the execution configuration, which you can see on the image above where we are defining that when we 
            launch performwork, we would like there to be two blocks in the grid with each of these blocks containing four threads. 
            However, when we define, for example, our performwork kernel, we would like to be able to indicate how the various threads and blocks 
            should be working together in their parallel execution to complete the work that we would like them to complete. </li>
        <li>To assist us in this process, inside of kernel definitions, we have some CUDA-provided variables that describe its executing thread, 
            block, and grid. Now in the same way that when you look at a function definition, you will see in the function definition's signature 
            some arguments that are expected to be passed in at runtime, and if you look inside the bottom of the function, you will see references 
            to those arguments that are defined in the function definition, but you don't know what the actual values of those arguments are until 
            the function is actually running and the program has passed concrete values in place of those arguments such that they can be used 
            inside the function definition. </li>
        <li>In a similar fashion, as we look at these special CUDA-provided thread hierarchy variables, we don't know what their actual value 
            is going to be until the kernel is launched and the work of the kernel is assigned to a particular thread within a particular block. 
            <ol>
                <li><b>gridDim.x</b>: The first special variable that we will look at is called <b>gridDim.x</b>, and gridDim.x is always going to be equal to the number of blocks 
                    in the grid. In the case of our example, we know from the image that we have two blocks in the grid, so if we had used the gridDim.x 
                    variable inside the definition for perform work, it would evaluate at runtime to the number two. </li>
                    <ul>I'll mention here the .x at the end of gridDim.x. We're going to be using the .x version of these variables throughout this section 
                        in the course. It's worth mentioning that we can also look at two- and three-dimensional grids and blocks, but we're not going to 
                        be worrying about that presently. We're just going to keep it simple as you're learning how to write CUDA kernels and work in one 
                        dimension only, namely the x dimension.</ul>
                <li><b>blockIdx.x</b>: The second variable we would like to look at is <b>blockIdx.x</b>, which will evaluate to the index of the current block within the grid 
                    where the kernel is executing. </li>
                <li><b>blockDim.x</b>: Inside kernel, we have another special variable called <b>blockDim.x</b>, and this describes the number 
                of threads in a block. So if we were to use this variable in our perform work kernel with the current launch configuration, 
                then blockDim.x would evaluate to the value of four, which describes the number of threads in a given block. </li>
                <li><b>threadIdx.x</b>: It's worth mentioning once again that all blocks in the grid will contain the same number of threads. Inside of a kernel, 
                we have one more variable that we'd like to look at, which is <b>threadIdx.x</b>, and this describes the index of the thread within 
                a block.  </li>
            </ol>

            <div class="code-tabs">
                <div class="tab-buttons">
                    <button class="tab-button" onclick="showCode('svu')">simple Variable usage</button>
                    <button class="tab-button" onclick="showCode('pex')">Parallel execution example</button>
                </div>
            
                <!-- CUDA Example Tab -->
                <div id="svu" class="tab-content">
                <pre><code class="language-c">
                #include <stdio.h>

                __global__ void printSuccessForCorrectExecutionConfiguration()
                {
                
                    if(threadIdx.x == 1 && blockIdx.x == 5)
                    {
                    printf("Success!\n");
                    } else {
                    printf("Failure. Update the execution configuration as necessary.\n");
                    }
                }
                
                int main()
                {
                
                    printSuccessForCorrectExecutionConfiguration<<<6, 6>>>();
                    cudaDeviceSynchronize();
                }
                </code></pre>
                </div>
        
            <!-- Model Definition Tab -->
            <div id="pex" class="tab-content">
            <pre><code class="language-python">
            #include stdio.h
            
            __global__ void loop()
            {
                /*
                * This kernel does the work of only 1 iteration
                * of the original for loop. Indication of which
                * "iteration" is being executed by this kernel is
                * still available via `threadIdx.x`.
                */
            
                printf("This is iteration number %d\n", threadIdx.x);
            }
            
            int main()
            {
            
                loop<<<1, 10>>>();
                cudaDeviceSynchronize();
            }
            </code></pre>
            </div>
    
        </div>


  <footer class="post-footer">

<!-- <nav class="paginav">
  <a class="prev" href="/posts/word_embedding/">
    <span class="title">« </span>
    <br>
    <span>Basic Word Embedding</span>
  </a>
  <a class="next" href="/posts/Transformers/">
    <span class="title"> »</span>
    <br>
    <span>Transformers?</span>
  </a>
</nav> -->



  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://siddharthaputti.github.io/">Sid&#39;IO</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
