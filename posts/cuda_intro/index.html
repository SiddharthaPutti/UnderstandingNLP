<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Low-Rank Adapters | Sid&#39;IO</title>
<meta name="keywords" content="nlp, language-model, alignment, steerability, prompting" />
<meta name="description" content="All this is going to be in lame/understandable language.">
<meta name="author" content="Siddhartha Putti">
<link rel="canonical" href="https://siddharthaputti.github.io//posts/LoRA/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.67a6fb6e33089cb29e856bcc95d7aa39f70049a42b123105531265a0d9f1258b.css" integrity="sha256-Z6b7bjMInLKehWvMldeqOfcASaQrEjEFUxJloNnxJYs=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://siddharthaputti.github.io/doge.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://siddharthaputti.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://siddharthaputti.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://siddharthaputti.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://siddharthaputti.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<style>
    .code-tabs {
        margin: 20px 0;
        border: 1px solid var(--border);
        border-radius: 4px;
    }
    
    .tab-buttons {
        display: flex;
        border-bottom: 1px solid var(--border);
        background: var(--entry);
    }
    
    .tab-button {
        padding: 10px 20px;
        border: none;
        background: none;
        color: var(--secondary);
        cursor: pointer;
        border-right: 1px solid var(--border);
    }
    
    .tab-button:hover {
        background: var(--tertiary);
    }
    
    .tab-button.active {
        background: var(--theme);
        color: var(--primary);
    }
    
    .tab-content {
        display: none;
        padding: 15px;
    }
    
    .tab-content.active {
        display: block;
    }
    
    .tab-content pre {
        margin: 0;
    }
    </style>
    
    <script>
    function showCode(tabId) {
        // Hide all tab contents
        document.querySelectorAll('.tab-content').forEach(tab => {
            tab.classList.remove('active');
        });
        
        // Deactivate all buttons
        document.querySelectorAll('.tab-button').forEach(button => {
            button.classList.remove('active');
        });
        
        // Show selected tab content
        document.getElementById(tabId).classList.add('active');
        
        // Activate selected button
        document.querySelector(`[onclick="showCode('${tabId}')"]`).classList.add('active');
    }
    </script>
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-HFT45VFBX6"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-HFT45VFBX6', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Low-Rank Adapters" />
<meta property="og:description" content="LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://siddharthaputti.github.io//posts/LoRA/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-15T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2023-03-15T00:00:00&#43;00:00" />

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://siddharthaputti.github.io//posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Word Embedding",
      "item": "https://siddharthaputti.github.io//posts/LoRA/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LoRA",
  "name": "LoRA",
  "description": "LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS",
  "keywords": [
    "nlp", "language-model", "alignment", "steerability", "prompting"
  ],
  "articleBody": "",
   "wordCount" : "4428",
  "inLanguage": "en",
  "datePublished": "2023-03-15T00:00:00Z",
  "dateModified": "2023-03-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Siddhartha Putti"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://siddharthaputti.github.io//posts/LoRA/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sid'IO",
    "logo": {
      "@type": "ImageObject",
      "url": "https://siddharthaputti.github.io/doge.png"
    }
  }
}

</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://siddharthaputti.github.io/" accesskey="h" title="Sid&#39;IO (Alt + H)">Sid&#39;IO</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://siddharthaputti.github.io/faq" title="FAQ">
                    <span>FAQ</span>
                </a>
            </li>
           
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      ABC's of CUDA.
    </h1>
    <div class="post-meta"><span title='2023-03-15 00:00:00 +0000 UTC'>Dec 17, 2023</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp; Siddhartha Putti
    </div>
    
    <h2 style="margin-top: 40px">GPU accelerated VS CPU-only Applications</h2>

<li>Let's begin with a discussion of some of the key differences between GPU accelerated as compared to CPU-only applications.</li>
<div style="margin: 20px 0">
    <img src="cpu_timeline.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
</div>
<li>The above picture, you can see a timeline that indicates work that can be done on CPU and representation of data that will need to initalized and and worked on in the application. 
    In CPU-only applications, data, of course, is allocated on the CPU after the data has been allocated and or initialized, we're then able to perform work on the CPU on that data. You can see, 
    based on the timeline, that the work is typically performed serially, which is to say one item of work will proceed After the previous item of work and so on and so forth. 
    If we have any downstream tasks after performing the work, such as verifying the results of our processing, then that work is also performed on the CPU. </li>

    <div style="margin: 20px 0">
        <img src="gpu_timeline.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
    </div>

    <li>we've made a modification now that we want to think about a GPU-accelerated application, and the way that we have to think about this is that 
        we have two separate devices that we're going to be working with. We have the traditional CPU, and we also have the GPU device. </li>
    <li>In a GPU-accelerated application, we can allocate our data, and that data will be initially available on the CPU. we are going to be 
        using a call to <b>cudaMallocManaged()</b> to allocate our data. Here, once we have allocated it with <b>cudaMallocManaged()</b>, we can still perform 
        functions such as the initialization of the values in the data on the CPU. </li>
    <li>However, now when we wish to perform work, we can indicate that we wish to perform the work on the GPU, where, as is represented 
        in this picture above, it will be performed largely in parallel. Having used <b>cudaMallocManaged()</b> to allocate the data, 
        the data will automatically be migrated to the GPU, where the parallel work can be done. </li>
    <li>Worth mentioning that work on the GPU is asynchronous to the work that could be happening on the CPU at the same time. 
        To represent that in this picture, we've shown that while the GPU is performing its work in parallel, it may be the case that 
        we wish to do some other work on the CPU at the same time. It's going to be important at times to synchronize our CPU code with 
        the completion of the work or some of the work that's happening on the GPU, and we can do that by creating synchronization barriers. 
        You'll be looking at a particular way to do that using a call to <b>cudaDeviceSynchronize()</b>. Let's assume for the sake of example that we 
        then wish to do some downstream verification of our GPU work on the CPU. Having used <b>cudaMallocManaged()</b> to allocate our data, the data 
        accesses by the CPU will automatically be migrated back to the CPU. </li>

<h2 style="margin-top: 40px">CUDA kernel execution</h2>
    <li>Now, let's look at some of the key terms and concepts associated with launching CUDA kernels, and some of the initial details to help you understand 
        how they perform their parallel work. In the previous section, we looked at when we perform work in the context of an accelerated application that 
        involves both a CPU and a GPU, and we showed that when we perform work, we can perform it in parallel on the GPU. Here, we're going to zoom in to 
        this call to a function we show here as PerformWork. </li> 

        <div style="margin: 20px 0">
            <img src="gpu-arch_1.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
        </div>

    <li>So, as mentioned, and as one of the great strengths of GPU parallel programming, GPUs are going to do work in a massively parallel way. 
        When we perform work on the GPU, the GPU work is done in something we call a thread. In this image, and in many of the images to follow, 
        we are representing a thread of work by one of these white rectangles. Many threads, in fact, on the GPU will run in a massively parallel fashion. 
        There is a way to logically group threads, and we call this logical collection of threads a block. </li> 
    <li>So, a block consists of one or more threads. We call a collection of all the blocks associated with work on the GPU a grid. 
        So, we have a grid of blocks of threads. When we refer to CUDA functions that run on the GPU, we have a special name for them. 
        They are called kernels. In this image, we have a kernel called PerformWork. When we invoke kernels, we say that we launch them, 
        and different from CPU-only functions, we launch them with something called an execution configuration, which you can see here in green. 
        It is this triple chevron syntax that we provide after the name of the function and before the parentheses, where we may need to pass arguments 
        to the function that it would be expected to have. </li> 
    <li>The execution configuration defines the number of blocks in the grid. This is the first argument that we pass into the execution configuration. 
        In this image, you can see that we are defining there should be two blocks in the grid. The execution configuration also defines the number of 
        threads that should be in each block. So, here we are stating that we should have four threads in each of our two blocks. 
        Worth mentioning is that every block in the grid will contain the same number of threads. </li>
    <li>To summarize, The kernel function is defined using the <b>__global__</b> keyword, and it takes in a variable number of arguments. The arguments are passed to the kernel
         function using the <b><<<>>></b> syntax, where the first argument is the number of blocks in the grid, and the second argument is the number of threads 
         in the block.</li>



         
         <div class="code-tabs">
            <div class="tab-buttons">
                <button class="tab-button active" onclick="showCode('cuda')">simple cuda kernel</button>
                <button class="tab-button" onclick="showCode('model')">Parallel CUDA kernel</button>
            </div>
        
            <!-- CUDA Example Tab -->
            <div id="cuda" class="tab-content active">
                <pre><code class="language-c">
        #include < stdio.h >
        
        void helloCPU()
        {
            printf("Hello from the CPU.\n");
        }
        
        __global__ void helloGPU()
        {
            printf("Hello from the GPU\n");
        }
        
        int main()
        {
            helloCPU();
            helloGPU<<<1, 1>>>();
            cudaDeviceSynchronize();
        }
                </code></pre>
        </div>
    
        <!-- Model Definition Tab -->
        <div id="model" class="tab-content">
            <pre><code class="language-python">
            #include < stdio.h >

            __global__ void firstParallel()
            {
                printf("This should be running in parallel.\n");
            }
            
            int main()
            {

                firstParallel<<<5, 5>>>(); # this will run 25 times in parallel. 
                cudaDeviceSynchronize();
            
            }
                </code></pre>
            </div>

        </div>

<h2 style="margin-top: 40px">CUDA provided thread hierarchy varaibles.</h2>
        <div style="margin: 20px 0">
            <img src="gpu_grid.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
        </div>

        <li>Now we're going to look at some CUDA-provided thread hierarchy variables that are made available to us by the CUDA runtime to use within 
            our CUDA kernel definitions and help control how we perform the parallel work that we will do with our kernel on the GPU. 
            We looked in a previous section at the execution configuration, which you can see on the image above where we are defining that when we 
            launch performwork, we would like there to be two blocks in the grid with each of these blocks containing four threads. 
            However, when we define, for example, our performwork kernel, we would like to be able to indicate how the various threads and blocks 
            should be working together in their parallel execution to complete the work that we would like them to complete. </li>
        <li>To assist us in this process, inside of kernel definitions, we have some CUDA-provided variables that describe its executing thread, 
            block, and grid. Now in the same way that when you look at a function definition, you will see in the function definition's signature 
            some arguments that are expected to be passed in at runtime, and if you look inside the bottom of the function, you will see references 
            to those arguments that are defined in the function definition, but you don't know what the actual values of those arguments are until 
            the function is actually running and the program has passed concrete values in place of those arguments such that they can be used 
            inside the function definition. </li>
        <li>In a similar fashion, as we look at these special CUDA-provided thread hierarchy variables, we don't know what their actual value 
            is going to be until the kernel is launched and the work of the kernel is assigned to a particular thread within a particular block. 
            <ol>
                <li><b>gridDim.x</b>: The first special variable that we will look at is called <b>gridDim.x</b>, and gridDim.x is always going to be equal to the number of blocks 
                    in the grid. In the case of our example, we know from the image that we have two blocks in the grid, so if we had used the gridDim.x 
                    variable inside the definition for perform work, it would evaluate at runtime to the number two. </li>
                    <ul>I'll mention here the .x at the end of gridDim.x. We're going to be using the .x version of these variables throughout this section 
                        in the course. It's worth mentioning that we can also look at two- and three-dimensional grids and blocks, but we're not going to 
                        be worrying about that presently. We're just going to keep it simple as you're learning how to write CUDA kernels and work in one 
                        dimension only, namely the x dimension.</ul>
                <li><b>blockIdx.x</b>: The second variable we would like to look at is <b>blockIdx.x</b>, which will evaluate to the index of the current block within the grid 
                    where the kernel is executing. </li>
                <li><b>blockDim.x</b>: Inside kernel, we have another special variable called <b>blockDim.x</b>, and this describes the number 
                of threads in a block. So if we were to use this variable in our perform work kernel with the current launch configuration, 
                then blockDim.x would evaluate to the value of four, which describes the number of threads in a given block. </li>
                <li><b>threadIdx.x</b>: It's worth mentioning once again that all blocks in the grid will contain the same number of threads. Inside of a kernel, 
                we have one more variable that we'd like to look at, which is <b>threadIdx.x</b>, and this describes the index of the thread within 
                a block.  </li>
            </ol>

            <div class="code-tabs">
                <div class="tab-buttons">
                    <button class="tab-button" onclick="showCode('svu')">simple Variable usage</button>
                    <button class="tab-button" onclick="showCode('pex')">Parallel execution example</button>
                </div>
            
                <!-- CUDA Example Tab -->
                <div id="svu" class="tab-content">
                <pre><code class="language-c">
                #include < stdio.h >

                __global__ void printSuccessForCorrectExecutionConfiguration()
                {
                
                    if(threadIdx.x == 1 && blockIdx.x == 5)
                    {
                    printf("Success!\n");
                    } else {
                    printf("Failure. Update the execution configuration as necessary.\n");
                    }
                }
                
                int main()
                {
                
                    printSuccessForCorrectExecutionConfiguration<<<6, 6>>>();
                    cudaDeviceSynchronize();
                }
                </code></pre>
                </div>
        
            <!-- Model Definition Tab -->
            <div id="pex" class="tab-content">
            <pre><code class="language-python">
            #include < stdio.h >
            
            __global__ void loop()
            {
                /*
                * This kernel does the work of only 1 iteration
                * of the original for loop. Indication of which
                * "iteration" is being executed by this kernel is
                * still available via `threadIdx.x`.
                */
            
                printf("This is iteration number %d\n", threadIdx.x);
            }
            
            int main()
            {
            
                loop<<<1, 10>>>();
                cudaDeviceSynchronize();
            }
            </code></pre>
            </div>
    
        </div>

<h2 style="margin-top: 40px">Co-ordinating Parallel Threads</h2>

    <div style="margin: 20px 0">
        <img src="data_assign.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
    </div>

    <li>Co-ordinating the work of parallel threads: Harking back to the previous sections, where we showed at a high level that we have some data that 
        has been migrated to the GPU, where we would like to perform some work in parallel on that data. So we're going to zoom in on these two sections 
        of our larger overarching diagram. Here we have, as we looked at in the previous section, our performwork kernel, which is being launched with 
        an execution configuration specifying two blocks of four threads each. And we're going to add to this image some data which has been migrated 
        to the GPU. And let's assume that the data is a zero-indexed vector, such that we could add some IDs to it to help us keep track of which value 
        is which. Now, of course, in real situations, we're going to be working with much larger datasets. But for the sake of learning, we can keep our 
        numbers smallish here. And it will be just fine for us to learn what we need to learn. </li>
    <li>So the idea, of course, is that when we write critical to execute on the GPU, we would like somehow for each of these threads, which are 
        executing in parallel, to be mapped to performwork on one of the elements in the vectors. There are certainly more sophisticated ways to 
        work on data. But to begin with, I think it's a great place to start when we do something that we call embarrassingly parallel, where each 
        of our threads is going to perform work on one item of data, and it's all going to happen in parallel. </li>
    <li>So how are we going to write a kernel definition, in this case for perform work, that is going to allow each of these individual threads 
        executing in parallel to perform work on one and only one of the items in our vector? So to figure this out, it's worth refreshing our 
        memories about a few of the special variables that we have access to in our kernels. First, recall that each thread has access to the 
        size of its block via the blockDim.x variable. It also has the index of its block within the grid via the blockIdx.x variable. 
        And finally, it has its own index within its block via the threadIdx.x variable. So here you can see we have specified an index for each 
        of the two blocks, indexes for each of the threads within the blocks, and also the dimension of each block, in this case four.</li>

    <div style="margin: 20px 0">
        <img src="formula.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
    </div>

    <li>Using these three variables, we have a very, very canonical formula that as you get exposed to more code, you're going to see very, very often. 
        And the formula is <b>threadIdx.x + blockIdx.x * blockDim.x</b>. This particular formula is going to map every thread within the entire grid to one 
        element in the vector. Another way that you can think about it is that this formula would give every thread a unique index among all the threads 
        throughout the entire grid.</li>

        <p><pre tabindex="0"><code> 
            # simple formula execution kernel
            #include < stdio.h >

            /*
                * Refactor `loop` to be a CUDA Kernel. The new kernel should
                * only do the work of 1 iteration of the original loop.
                */
            
            __global__ void loop()
            {
                printf("This is iteration number %d\n", threadIdx.x + blockIdx.x * blockDim.x);
            }
            
            int main()
            {
                /*
                * When refactoring `loop` to launch as a kernel, be sure
                * to use the execution configuration to control how many
                * "iterations" to perform.
                *
                * For this exercise, be sure to use more than 1 block in
                * the execution configuration.
                */
            
                
                loop<<<2, 5>>>();
                cudaDeviceSynchronize();
            }
        </code></pre></p>

<h2 style="margin-top: 40px">Grid Size mismatch</h2>

        <li>we're going to look at grid-size work amount mismatch. To say more on that, in the previous scenarios, as we discussed, the number of threads 
            in the grid happened to match exactly the number of elements in the data that we wanted to work on. Now, for any number of reasons, this may 
            not actually be the case for our program, so what if there are more threads in our grid than the amount of work that needs to be done? 
            If we used the exact same setup that we used in the previous section, then we would be attempting to access non-existent elements, and 
            this could definitely result in a runtime error and or other unintended consequences in our code. </li>

            <div style="margin: 20px 0">
                <img src="mismatch.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
            </div>

        <li>So, it's really quite straightforward. It's just that our code needs to check that the data index we calculate through our equation 
            <b>threadIdx.x + blockIdx.x * blockDim.x</b> is actually less than N, which would be the number of data elements. So, with that in mind, 
            if we know that N, in this case, is 5, because we have 5 data elements to work on, then we can extend our equation a little bit and 
            perform a check that states that the result of our initial equation, which gives us our data index, needs to actually be less than the 
            value of N. And if we look at this Thread0 in Block1 that's highlighted in green, if we run this check, we would see that the data index 4 
            is less than N, which is 5, so it's fine, you can do work, and it would work on this element over here. However, for the next thread 
            in the block, we would see that 5 is not less than 5, so we would not allow this particular thread to perform work, and same with the 
            remaining threads in this block. </li> 

<h2 style="margin-top: 40px">Grid-Stride Loop</h2>
        <li>In this section, we're going to talk about a very common strategy we employ in our Cougar kernels, called a grid-stripe loop. 
            So, very commonly, there are more data elements than there are threads in the grid. In this particular example, we have the imaginary grid 
            that we are launching with the PerformWork kernel, which has two blocks of four threads each, giving us a total of eight threads. 
            And in this example, we have a data vector that has 32 elements. So, if we were to employ the strategy that we've been doing thus far, 
            where we had one thread working on one data element, this isn't going to work for us, because we're going to have a lot of work in the data 
            that's left undone. So, one very common way to address this programmatically is with the creation of a grid-stripe loop. </li>

            <div style="margin: 20px 0"></div>
                <img src="stride-loop.png" alt="CPU Timeline showing serial processing" style="max-width: 100%; height: auto;">
            </div>
        <li>So, when we construct a grid-stripe loop inside of our kernel definition, we are going to construct a loop, as the name might suggest, 
            where the thread's first element is calculated per usual with our <b>threadIdx + blockIdx * blockDim</b> calculation, giving that thread a unique 
            index for all threads in the grid, and in this way, it will do its computation first on a single element. Now, within this loop that we're 
            constructing, when we want to increment the value that maps to the data index, we are going to stride by the total number of threads in the 
            grid. And we can ascertain the total number of threads in the grid by doing <b>blockDim.x * gridDim.x</b>. </li>
        <li>In this situation, the block dimension is four, and the grid dimension is two, so that will give us a value of eight. So, in this way, 
            the thread would continue to iterate through the loop. We would conclude the loop when the index is greater than the number of data 
            elements that we have, and the thread would perform work on several different elements in the data. So, under the assumption that our 
            kernel has been defined, then all the threads will be working in this similar way, and you can see, if we were to display this, 
            that as each of the threads performs the same iteration, it would result in all elements of the data being covered. </li>
  
            <div class="code-tabs">
                <div class="tab-buttons">
                    <button class="tab-button" onclick="showCode('gsm')">Grid size mismatch</button>
                    <button class="tab-button" onclick="showCode('gsm2')">Grid size mismatch 2</button>
                    <button class="tab-button" onclick="showCode('stl')">Stride Loop Example</button>
                </div>
            
                <!-- CUDA Example Tab -->
                <div id="gsm" class="tab-content">
                    <pre><code class="language-c">
                    #include < stdio.h >

                    void init(int *a, int N)
                    {
                        int i;
                        for (i = 0; i < N; ++i)
                        {
                        a[i] = i;
                        }
                    }
                    
                    __global__
                    void doubleElements(int *a, int N)
                    {
                        int i;
                        i = blockIdx.x * blockDim.x + threadIdx.x;
                        if (i < N)
                        {
                        a[i] *= 2;
                        }
                    }
                    
                    bool checkElementsAreDoubled(int *a, int N)
                    {
                        int i;
                        for (i = 0; i < N; ++i)
                        {
                        if (a[i] != i*2) return false;
                        }
                        return true;
                    }
                    
                    int main()
                    {
                        int N = 1000;
                        int *a;
                    
                        size_t size = N * sizeof(int);
                    
                        /*
                        * Use `cudaMallocManaged` to allocate pointer `a` available
                        * on both the host and the device.
                        */
                    
                        cudaMallocManaged(&a, size);
                    
                        init(a, N);
                    
                        size_t threads_per_block = 256;
                        size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;
                    
                        doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);
                        cudaDeviceSynchronize();
                    
                        bool areDoubled = checkElementsAreDoubled(a, N);
                        printf("All elements were doubled? %s\n", areDoubled ? "TRUE" : "FALSE");
                    
                        /*
                        * Use `cudaFree` to free memory allocated
                        * with `cudaMallocManaged`.
                        */
                    
                        cudaFree(a);
                    }
                    </code></pre>
            </div>
        
            <!-- Model Definition Tab -->
            <div id="gsm2" class="tab-content">
                <pre><code class="language-python">
                #include < stdio.h >

                /*
                    * Currently, `initializeElementsTo`, if executed in a thread whose
                    * `i` is calculated to be greater than `N`, will try to access a value
                    * outside the range of `a`.
                    *
                    * Refactor the kernel definition to prevent out of range accesses.
                    */
                
                __global__ void initializeElementsTo(int initialValue, int *a, int N)
                {
                    int i = threadIdx.x + blockIdx.x * blockDim.x;
                    if (i < N) {
                    a[i] = initialValue;
                }
                }
                
                int main()
                {
                    /*
                    * Do not modify `N`.
                    */
                
                    int N = 1000;
                
                    int *a;
                    size_t size = N * sizeof(int);
                
                    cudaMallocManaged(&a, size);
                
                    /*
                    * Assume we have reason to want the number of threads
                    * fixed at `256`: do not modify `threads_per_block`.
                    */
                
                    size_t threads_per_block = 256;
                
                    /*
                    * Assign a value to `number_of_blocks` that will
                    * allow for a working execution configuration given
                    * the fixed values for `N` and `threads_per_block`.
                    */
                
                    size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;
                
                    int initialValue = 6;
                
                    initializeElementsTo<<< number_of_blocks, threads_per_block >>>(initialValue, a, N);
                    cudaDeviceSynchronize();
                
                    /*
                    * Check to make sure all values in `a`, were initialized.
                    */
                
                    for (int i = 0; i < N; ++i)
                    {
                    if(a[i] != initialValue)
                    {
                        printf("FAILURE: target value: %d\t a[%d]: %d\n", initialValue, i, a[i]);
                        cudaFree(a);
                        exit(1);
                    }
                    }
                    printf("SUCCESS!\n");
                
                    cudaFree(a);
                    }
                    </code></pre>
                </div>

                <div id="stl" class="tab-content">
                    <pre><code class="language-python">
                    #include < stdio.h >

                    void init(int *a, int N)
                    {
                        int i;
                        for (i = 0; i < N; ++i)
                        {
                        a[i] = i;
                        }
                    }
                    
                    /*
                        * In the current application, `N` is larger than the grid.
                        * Refactor this kernel to use a grid-stride loop in order that
                        * each parallel thread work on more than one element of the array.
                        */
                    
                    __global__
                    void doubleElements(int *a, int N)
                    {
                        int idx = blockIdx.x * blockDim.x + threadIdx.x;
                        int stride = gridDim.x * blockDim.x;
                        for (int i = idx; i < N; i += stride){
                        a[i] *=2;
                        }
                    }
                    
                    bool checkElementsAreDoubled(int *a, int N)
                    {
                        int i;
                        for (i = 0; i < N; ++i)
                        {
                        if (a[i] != i*2) return false;
                        }
                        return true;
                    }
                    
                    int main()
                    {
                        /*
                        * `N` is greater than the size of the grid (see below).
                        */
                    
                        int N = 10000;
                        int *a;
                    
                        size_t size = N * sizeof(int);
                        cudaMallocManaged(&a, size);
                    
                        init(a, N);
                    
                        /*
                        * The size of this grid is 256*32 = 8192.
                        */
                    
                        size_t threads_per_block = 256;
                        size_t number_of_blocks = 32;
                    
                        doubleElements<<<number_of_blocks, threads_per_block>>>(a, N);
                        cudaDeviceSynchronize();
                    
                        bool areDoubled = checkElementsAreDoubled(a, N);
                        printf("All elements were doubled? %s\n", areDoubled ? "TRUE" : "FALSE");
                    
                        cudaFree(a);
                    }
                        </code></pre>
                    </div>
    
            </div>

<h3 style="margin-top: 40px">Req's</h3>

<li>The kernel should be defined using the __global__ keyword.</li>
<li>The kernel should take in a variable number of arguments.</li>
<li>The arguments should be passed to the kernel function using the <<<>>> syntax.</li>
<li>The first argument is the number of blocks in the grid.</li>
<li>The second argument is the number of threads in each block.</li>
<li>The kernel should be executed using the cudaDeviceSynchronize() function.</li>
<li>The code file should be in .cu format.</li>
<li>The code file can be executed using nvcc compiler: "nvcc -o output.exe program_name.cu" and then "./output.exe" to run the program.</li>
<li> you can install nvcc using "sudo apt-get install nvidia-cuda-toolkit" on ubuntu.</li>
  
  
  
        <footer class="post-footer">

<!-- <nav class="paginav">
  <a class="prev" href="/posts/word_embedding/">
    <span class="title">« </span>
    <br>
    <span>Basic Word Embedding</span>
  </a>
  <a class="next" href="/posts/Transformers/">
    <span class="title"> »</span>
    <br>
    <span>Transformers?</span>
  </a>
</nav> -->



  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://siddharthaputti.github.io/">Sid&#39;IO</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
